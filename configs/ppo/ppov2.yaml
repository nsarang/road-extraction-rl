version: 2.1.0

experiment:
  name: 31-ppo-discrete
  engine: PPO
  root_dir: experiments
  dir: null # AUTOMATICALLY SET

env:
  train:
    data:
      cities:
        - toronto
        - paris
        - la
        - chicago
        - houston
      dir: data

    action_scheme:
      name: GraphDiscreteRetrace
      args:
        step_size: 10
        num_directions: 32

    reward_scheme:
      name: DistReward
      args:
        neutral_distance: 5
        revisit_distance: 20
        max_distance: 35
        exponent: 1
        normalize: false
        nearest_unvisited_len_thresh: 40 
        min_viable_path_len: 20
        apply_road_width: false

    stopper:
      name: WindowAverage
      args:
        min_average: -10
        window_length: 7
        gameover_multiplier: 1.5

    renderer: # list supported
      VideoRenderer9000:
        frame_size:
          - 1000
          - 1000
        gamma: ${policy.ppo.discount_factor}
        fps: 4
        max_frames: 200
      TopViewRenderer:
        scale: 0.25
        fps: 30
        skip_rate: 0.5
        min_frames: 30

    base:
      observation_shape:
        - 512
        - 512
      starting_location: null
      observation_mode: rgb+movement
      experiment_dir: ${experiment.dir}

    wrappers:
      UndoW:
        num_undo: 15
        hard_reset_rate: 0.10
      EpisodicEnvW:
        max_steps: 200
        max_episodes: 50
      AutoSaveW:
        mode: episode
        period: 150
      ObservationAugmentationW:
        mode: train
        output_shape:
          - 256
          - 256
      InputRepresentationW:
        mode: stacked

  test:
    data:
      cities:
        - kansas city
        - montreal
      dir: ${env.train.data.dir}

    action_scheme: ${env.train.action_scheme}
    reward_scheme: ${env.train.reward_scheme}

    stopper:
      name: WindowAverage
      args:
        min_average: -15
        window_length: 12
        gameover_multiplier: ${env.train.stopper.args.gameover_multiplier}

    renderer: ${env.train.renderer}

    base:
      starting_location: null
      observation_shape: ${env.train.base.observation_shape}
      observation_mode: ${env.train.base.observation_mode}
      experiment_dir: ${experiment.dir}
    wrappers:
      EpisodicEnvW:
        max_steps: 5000
        max_episodes: 10
      AutoSaveW:
        mode: episode
        period: 3
        start: 6
      ObservationAugmentationW:
        mode: test
        output_shape: ${env.train.wrappers.ObservationAugmentationW.output_shape}
      InputRepresentationW: ${env.train.wrappers.InputRepresentationW}


collector:
  num_train_envs: 5
  num_test_envs: 2
  train_seed: 11
  test_seed: 111
  buffer_type: PERB
  buffer_size: 2000
  alpha: 0.8
  beta: 0.5

policy:
  device: dp
  backbone:
    name: core.policy.networks.EfficientNetEx
    args:
      name: efficientnet-b0
      feature_vector_size: 1000
      obs_channels: 4
      pretrained: true
  optimizer:
    lr: 5e-4
    weight_decay: 1e-4
  ppo:
    discount_factor: 0.5
    vf_coef: 0.4
    ent_coef: 0.1
    eps_clip: 0.2
    max_grad_norm: 50
    gae_lambda: 0.95
    reward_normalization: true
    advantage_normalization: true
    dual_clip: null
    value_clip: true
    max_batchsize: 64
checkpoint: null


trainer:
  args:
    max_epoch: 1000
    step_per_epoch: 10000
    repeat_per_collect: 6
    episode_per_test: 12
    batch_size: 64
    step_per_collect: 500
  checkpointer:
    monitor: rew_avg
    filename: policy_{epoch}_{rew_avg:.2f}
    save_last: true
    save_top_k: 3
    mode: max
    verbose: true
  experiment_dir: ${experiment.dir}
  reward_threshold: 1000